{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb6741",
   "metadata": {},
   "source": [
    "#### Private Encoder (For Source and Target Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivateEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=1320, hidden_dim=512, num_layers=4):\n",
    "        super(PrivateEncoder, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad8b8e",
   "metadata": {},
   "source": [
    "#### Shared Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b40d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=1320, hidden_dim=1024, num_layers=6):\n",
    "        super(SharedEncoder, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb197522",
   "metadata": {},
   "source": [
    "#### Senone Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenoneClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=1024, output_dim=3080):\n",
    "        super(SenoneClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5de9d5",
   "metadata": {},
   "source": [
    "#### Domain Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f23167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # source or target\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.domain_classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d3b71",
   "metadata": {},
   "source": [
    "#### Shared Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3214881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=1024, output_dim=1320):\n",
    "        super(SharedDecoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0f2d5",
   "metadata": {},
   "source": [
    "#### DSN Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DSN, self).__init__()\n",
    "        self.shared_encoder = SharedEncoder()\n",
    "        self.private_encoder_source = PrivateEncoder()\n",
    "        self.private_encoder_target = PrivateEncoder()\n",
    "        self.senone_classifier = SenoneClassifier()\n",
    "        self.domain_classifier = DomainClassifier()\n",
    "        self.shared_decoder = SharedDecoder()\n",
    "\n",
    "    def forward(self, x, domain='source', mode='train'):\n",
    "        private_encoder = self.private_encoder_source if domain == 'source' else self.private_encoder_target\n",
    "        private_feat = private_encoder(x)\n",
    "        shared_feat = self.shared_encoder(x)\n",
    "\n",
    "        if mode == 'train':\n",
    "            recon = self.shared_decoder(shared_feat)\n",
    "            senone_out = self.senone_classifier(shared_feat)\n",
    "            domain_out = self.domain_classifier(shared_feat)\n",
    "            return private_feat, shared_feat, recon, senone_out, domain_out\n",
    "        elif mode == 'inference':\n",
    "            senone_out = self.senone_classifier(shared_feat)\n",
    "            return senone_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d411ab",
   "metadata": {},
   "source": [
    "### Loss Function and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fe738",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.25  # reconstruction loss\n",
    "gamma = 0.075  # domain classification loss\n",
    "delta = 0.1  # difference loss\n",
    "\n",
    "criterion_recon = nn.MSELoss()\n",
    "criterion_domain = nn.CrossEntropyLoss()\n",
    "criterion_senone = nn.CrossEntropyLoss()\n",
    "\n",
    "def difference_loss(private, shared):\n",
    "    return torch.mean(torch.sum((F.normalize(private, dim=1) * F.normalize(shared, dim=1))**2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = DSN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14221e64",
   "metadata": {},
   "source": [
    "source_feats.npy: shape (15000, 1320)\n",
    "\n",
    "source_labels.npy: shape (15000,) — senone labels\n",
    "\n",
    "target_feats.npy: shape (2837, 1320) — target domain has no labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f342fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load Kaldi-extracted features and labels\n",
    "source_feats = torch.tensor(np.load(\"source_feats.npy\"), dtype=torch.float32)     # (15000, 1320)\n",
    "source_labels = torch.tensor(np.load(\"source_labels.npy\"), dtype=torch.long)      # (15000,)\n",
    "target_feats = torch.tensor(np.load(\"target_feats.npy\"), dtype=torch.float32)     # (2837, 1320)\n",
    "\n",
    "# Wrap into Datasets\n",
    "source_dataset = TensorDataset(source_feats, source_labels)\n",
    "# Target domain has dummy labels just to allow zipping\n",
    "target_dataset = TensorDataset(target_feats, torch.zeros(len(target_feats)))\n",
    "\n",
    "# Build DataLoaders\n",
    "batch_size = 128\n",
    "source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97a002",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DSN().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.95)\n",
    "\n",
    "# Dummy loop\n",
    "for epoch in range(20):\n",
    "    for (source_x, source_y), (target_x, _) in zip(source_loader, target_loader):\n",
    "        source_x, source_y = source_x.to(device), source_y.to(device)\n",
    "        target_x = target_x.to(device) \n",
    "\n",
    "        # Forward for source\n",
    "        priv_src, shared_src, recon_src, senone_out, domain_out_src = model(source_x, domain='source', mode='train')\n",
    "        loss_senone = criterion_senone(senone_out, source_y)\n",
    "        loss_domain_src = criterion_domain(domain_out_src, torch.zeros(source_x.size(0), dtype=torch.long, device=device))\n",
    "        loss_recon_src = criterion_recon(recon_src, source_x)\n",
    "\n",
    "        # Forward for target\n",
    "        priv_tgt, shared_tgt, recon_tgt, _, domain_out_tgt = model(target_x, domain='target', mode='train')\n",
    "        loss_domain_tgt = criterion_domain(domain_out_tgt, torch.ones(target_x.size(0), dtype=torch.long, device=device))\n",
    "        loss_recon_tgt = criterion_recon(recon_tgt, target_x)\n",
    "\n",
    "        # Difference loss\n",
    "        loss_diff_src = difference_loss(priv_src, shared_src)\n",
    "        loss_diff_tgt = difference_loss(priv_tgt, shared_tgt)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_senone \\\n",
    "               + beta * (loss_recon_src + loss_recon_tgt) \\\n",
    "               + gamma * (loss_domain_src + loss_domain_tgt) \\\n",
    "               + delta * (loss_diff_src + loss_diff_tgt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# After your training loop finishes (after scheduler.step() outside the loop)\n",
    "torch.save(model.state_dict(), \"dsn_model.pth\")\n",
    "print(\"Model saved to dsn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load test features\n",
    "test_feats = torch.tensor(np.load(\"test_feats.npy\"), dtype=torch.float32)  # shape: (558, 1320)\n",
    "\n",
    "# Wrap into a dataset (no labels needed for inference)\n",
    "test_dataset = TensorDataset(test_feats)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model weights before inference\n",
    "model.load_state_dict(torch.load(\"dsn_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6544a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (test_x,) in test_loader:\n",
    "        test_x = test_x.to(device)\n",
    "        logits = model(test_x, mode='inference')\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "np.save(\"test_predictions.npy\", all_preds.numpy())\n",
    "print(\"Inference predictions saved to test_predictions.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
